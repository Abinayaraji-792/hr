# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oauMl5hibA1Rh6N92Dfm1hBgQEzLvtuR
"""

# --------------------------------------------
# HR ATTRITION ANALYSIS PROJECT - STREAMLIT VERSION
# --------------------------------------------

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

# --------------------------------------------
# Streamlit UI
# --------------------------------------------
st.title("üíº Employee Attrition Prediction (Machine Learning Project)")
st.write("""
This Streamlit app runs your **HR Attrition Analysis pipeline** ‚Äî including:
data cleaning, feature engineering, model training, evaluation, tuning, and feature importance.
""")

# --------------------------------------------
# Step 2: Upload Dataset
# --------------------------------------------
st.subheader("üìÇ Step 1: Upload Your HR Dataset (.csv)")
uploaded_file = st.file_uploader("Upload your HR dataset", type=["csv"])

if uploaded_file is not None:
    df = pd.read_csv(uploaded_file)
    st.success(f"‚úÖ Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns.")
    st.write("### First 5 Rows of the Dataset")
    st.dataframe(df.head())

    # --------------------------------------------
    # Step 3: Check Data
    # --------------------------------------------
    st.write("### üìä Data Overview")
    st.write("**Data types:**")
    st.write(df.dtypes)
    st.write("**Missing Values:**")
    st.write(df.isnull().sum())
    st.write("**Attrition Distribution:**")
    st.write(df['Attrition'].value_counts(normalize=True))

    # --------------------------------------------
    # Step 4: Data Cleaning
    # --------------------------------------------
    df.drop_duplicates(inplace=True)
    st.write(f"‚úÖ Duplicates removed. New shape: {df.shape}")

    # --------------------------------------------
    # Step 5: Feature Engineering
    # --------------------------------------------
    np.random.seed(42)
    df['YearsSinceLastPromotion'] = np.maximum(0, df['YearsAtCompany'] - np.random.randint(0, 5, size=len(df)))
    df['OverTime_Hours'] = df['OverTime'].apply(lambda x: np.random.randint(5, 15) if x == 'Yes' else 0)
    df['Salary_per_Year'] = (df['Salary'] / (df['YearsAtCompany'] + 1)).round(2)

    st.write("‚úÖ New features added: `YearsSinceLastPromotion`, `OverTime_Hours`, `Salary_per_Year`")

    # --------------------------------------------
    # Step 6: Encoding and Splitting
    # --------------------------------------------
    X = df.drop('Attrition', axis=1)
    y = df['Attrition']

    label_enc = LabelEncoder()
    y = label_enc.fit_transform(y)  # Yes -> 1, No -> 0

    cat_cols = ['Department', 'OverTime', 'Education']
    num_cols = [col for col in X.columns if col not in cat_cols]

    preprocessor = ColumnTransformer([
        ('num', StandardScaler(), num_cols),
        ('cat', OneHotEncoder(drop='first'), cat_cols)
    ])

    # --------------------------------------------
    # Step 7: Train-Test Split
    # --------------------------------------------
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    st.success(f"‚úÖ Data Split Done! Train size: {X_train.shape}, Test size: {X_test.shape}")

    # --------------------------------------------
    # Step 8: Model Training and Evaluation
    # --------------------------------------------
    st.subheader("‚öôÔ∏è Step 2: Model Training & Evaluation")

    models = {
        "Logistic Regression": LogisticRegression(max_iter=1000),
        "Decision Tree": DecisionTreeClassifier(random_state=42),
        "Random Forest": RandomForestClassifier(random_state=42)
    }

    results = {}

    for name, model in models.items():
        pipe = Pipeline([('preprocess', preprocessor), ('model', model)])
        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)
        results[name] = {
            'Accuracy': accuracy_score(y_test, y_pred),
            'Precision': precision_score(y_test, y_pred),
            'Recall': recall_score(y_test, y_pred),
            'F1-Score': f1_score(y_test, y_pred),
            'ROC-AUC': roc_auc_score(y_test, y_pred)
        }

    st.write("### üìà Model Evaluation Results")
    results_df = pd.DataFrame(results).T
    st.dataframe(results_df.style.format("{:.3f}"))

    # --------------------------------------------
    # Step 9: Hyperparameter Tuning (Random Forest)
    # --------------------------------------------
    st.subheader("üîç Step 3: Hyperparameter Tuning (Random Forest)")

    param_grid = {
        'model__n_estimators': [100, 200],
        'model__max_depth': [5, 10, None]
    }

    rf_pipeline = Pipeline([
        ('preprocess', preprocessor),
        ('model', RandomForestClassifier(random_state=42))
    ])

    grid = GridSearchCV(rf_pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1)
    grid.fit(X_train, y_train)

    st.success("‚úÖ Grid Search Completed!")
    st.write(f"**Best Parameters:** {grid.best_params_}")
    st.write(f"**Best Accuracy:** {grid.best_score_:.3f}")

    # --------------------------------------------
    # Step 10: Model Interpretation (Feature Importance)
    # --------------------------------------------
    st.subheader("üí° Step 4: Feature Importance")

    final_model = grid.best_estimator_['model']
    ohe = grid.best_estimator_['preprocess'].named_transformers_['cat']
    encoded_cat = list(ohe.get_feature_names_out(cat_cols))
    all_features = num_cols + encoded_cat
    importances = final_model.feature_importances_

    feat_imp = pd.DataFrame({'Feature': all_features, 'Importance': importances})
    feat_imp.sort_values(by='Importance', ascending=False, inplace=True)

    fig, ax = plt.subplots(figsize=(10, 5))
    ax.barh(feat_imp['Feature'], feat_imp['Importance'])
    ax.invert_yaxis()
    plt.title("Feature Importance in Employee Attrition Prediction")
    plt.xlabel("Importance")
    st.pyplot(fig)

    st.write("### üîù Top Influential Features")
    st.dataframe(feat_imp.head(10))

    # --------------------------------------------
    # Step 11: Save Processed Dataset
    # --------------------------------------------
    df.to_csv("hr_dataset_processed.csv", index=False)
    st.success("‚úÖ Processing complete! File 'hr_dataset_processed.csv' created successfully.")

    st.download_button(
        label="üì• Download Processed Dataset",
        data=df.to_csv(index=False).encode('utf-8'),
        file_name='hr_dataset_processed.csv',
        mime='text/csv'
    )

else:
    st.info("üëÜ Please upload your HR dataset (.csv) file to begin the analysis.")